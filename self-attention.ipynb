{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a47cabb-0554-4bdb-868f-7feecf391915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'If': 0, 'attainable': 1, 'by': 2, 'humanly': 3, 'is': 5, 'it': 6, 'possible': 7, 'something': 8, 'too': 9, 'you': 10}\n"
     ]
    }
   ],
   "source": [
    "sentence = 'If something is humanly possible, it is attainable by you too'\n",
    "\n",
    "# dictionary to map each word to a unique integer id\n",
    "dict = {s:i for i,s in enumerate(sorted(sentence.replace(',', '').split()))}\n",
    "\n",
    "print(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ada8404-5625-4970-9eb4-303d0c3ab8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  8,  5,  3,  7,  6,  5,  1,  2, 10,  9])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "sentence_int = torch.tensor(\n",
    "    [dict[s] for s in sentence.replace(',', '').split()])\n",
    "\n",
    "print (sentence_int) # a tensor of integer IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67c3b220-df15-40f1-920f-db0661dac783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3374, -0.1778, -0.3035],\n",
      "        [ 0.4965, -1.5723,  0.9666],\n",
      "        [ 0.2692, -0.0770, -1.0205],\n",
      "        [-1.1925,  0.6984, -1.4097],\n",
      "        [ 1.3010,  1.2753, -0.2010],\n",
      "        [-0.1690,  0.9178,  1.5810],\n",
      "        [ 0.2692, -0.0770, -1.0205],\n",
      "        [-0.5880,  0.3486,  0.6603],\n",
      "        [-0.2196, -0.3792,  0.7671],\n",
      "        [-0.6315, -2.8400, -1.3250],\n",
      "        [-1.1481, -1.1589,  0.3255]])\n",
      "torch.Size([11, 3])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 50000\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embed = torch.nn.Embedding(vocab_size,3)\n",
    "embedded_sentence = embed(sentence_int).detach()\n",
    "\n",
    "print(embedded_sentence)       # embedding layer is kind of a look-up table. it turns word IDs into dense vectors\n",
    "print(embedded_sentence.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cae39aea-ddad-43bb-a5ec-f3f4c4f68514",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "d = embedded_sentence.shape[1]     #input embedding dimension\n",
    "\n",
    "dq, dk, dv = 2, 2, 4     # output dimensions for q, k, v\n",
    "\n",
    "Wquery = torch.nn.Parameter(torch.rand(d, dq))    # learnable weight matrices for query,key and value\n",
    "Wkey = torch.nn.Parameter(torch.rand(d,dk))\n",
    "Wvalue = torch.nn.Parameter(torch.rand(d, dv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aad4d343-388a-4246-a3a0-f054a9911e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.2961, 0.5166],\n",
      "        [0.2517, 0.6886],\n",
      "        [0.0740, 0.8665]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.1366, 0.1025],\n",
      "        [0.1841, 0.7264],\n",
      "        [0.3153, 0.6871]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.0756, 0.1966, 0.3164, 0.4017],\n",
      "        [0.1186, 0.8274, 0.3821, 0.6605],\n",
      "        [0.8536, 0.5932, 0.6367, 0.9826]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print (Wquery)\n",
    "print(Wkey)\n",
    "print(Wvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2eddaa83-8d13-483a-9ffa-85e1a807f4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2])\n",
      "torch.Size([2])\n",
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "# projecting embeddings for a single word into a query, key and value vector\n",
    "x_2 = embedded_sentence[1]\n",
    "query_2 = x_2 @ Wquery\n",
    "key_2 = x_2 @ Wkey\n",
    "value_2 = x_2 @ Wvalue\n",
    "\n",
    "print(query_2.shape)\n",
    "print(key_2.shape)\n",
    "print(value_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80c7a86f-c737-4a0b-b22e-56d79a872992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([11, 2])\n",
      "values.shape: torch.Size([11, 4])\n"
     ]
    }
   ],
   "source": [
    "# now doing the same for the entire sentence\n",
    "keys = embedded_sentence @ Wkey\n",
    "values = embedded_sentence @ Wvalue\n",
    "\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cb374fd-8fe7-4c12-a77d-cbf2f5a168c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0514, grad_fn=<DotBackward0>)\n"
     ]
    }
   ],
   "source": [
    "omega_24 = query_2.dot(keys[4]) #omega (w) is the unnormalized attn weight / attention score / measure of similarity b/w query and key\n",
    "print(omega_24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4faff4f3-d2ed-46e5-a4d4-a8c009db7e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0111, -0.0196,  0.0447,  0.0782, -0.0514, -0.0944,  0.0447, -0.0267,\n",
      "        -0.0226,  0.1473,  0.0390], grad_fn=<SqueezeBackward4>)\n"
     ]
    }
   ],
   "source": [
    "# calculating attention score for a single word (secodn word) against all words in the sentence\n",
    "omega_2 = query_2 @ keys.T \n",
    "print(omega_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86efabdf-8453-49ee-8951-9818a1f1f3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0907, 0.0887, 0.0928, 0.0951, 0.0867, 0.0841, 0.0928, 0.0883, 0.0885,\n",
      "        0.0998, 0.0925], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "attention_weights_2 = F.softmax(omega_2 / dk**0.5, dim=0)   # scale attn scores(omega) and convert them into a probability distribution\n",
    "\n",
    "print(attention_weights_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "222c60a2-c1cd-4854-bf8f-23b8a8107683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4])\n",
      "tensor([-0.1538, -0.3559, -0.2394, -0.3801], grad_fn=<SqueezeBackward4>)\n"
     ]
    }
   ],
   "source": [
    "context_vector_2 = attention_weights_2 @ values    # this vector(for 2nd word) is a weighted sum of the value vectors from all words\n",
    "print(context_vector_2.shape)\n",
    "print(context_vector_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df2444ea-830c-4480-98e6-4b7d834b2c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapping the entire SelfAttention mechanism in a class\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "\n",
    "    def __init__ (self, d_in, d_out_qk, d_out_v):\n",
    "        super().__init__()\n",
    "        self.d_out_qk = d_out_qk\n",
    "        self.Wquery = nn.Parameter(torch.rand(d_in, d_out_qk))\n",
    "        self.Wkey = nn.Parameter(torch.rand(d_in, d_out_qk))\n",
    "        self.Wvalue = nn.Parameter(torch.rand(d_in, d_out_v))\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.Wkey\n",
    "        queries = x @ self.Wquery\n",
    "        values = x @ self.Wvalue\n",
    "\n",
    "        attn_scores = queries @ keys.T  # unnormalised attention weights\n",
    "        attn_weights = torch.softmax(attn_scores / self.d_out_qk**0.5, dim=-1)\n",
    "\n",
    "        context_vector = attn_weights @ values\n",
    "        return context_vector\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ba61553-5c8d-48e9-b024-9eb161462ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2361, -0.5167, -0.3465, -0.5528],\n",
      "        [-0.1538, -0.3559, -0.2394, -0.3801],\n",
      "        [-0.6448, -1.3095, -0.8819, -1.4126],\n",
      "        [-1.0799, -2.2338, -1.4854, -2.3864],\n",
      "        [ 0.7037,  0.9007,  0.6967,  1.1096],\n",
      "        [ 0.8501,  1.0934,  0.8401,  1.3405],\n",
      "        [-0.6448, -1.3095, -0.8819, -1.4126],\n",
      "        [ 0.1525,  0.1379,  0.1183,  0.1896],\n",
      "        [ 0.0212, -0.0659, -0.0313, -0.0484],\n",
      "        [-1.5055, -3.2335, -2.1128, -3.4056],\n",
      "        [-1.0046, -2.0491, -1.3719, -2.2012]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "d_in, d_out_qk, d_out_v = 3,2,4\n",
    "\n",
    "sa = SelfAttention(d_in, d_out_qk, d_out_v)\n",
    "print(sa(embedded_sentence))          #look at the second row and it exactly matches the value from our calculations previously"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
