{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb45b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints\n",
    "(https://arxiv.org/pdf/2305.13245)\n",
    "\n",
    "Grouped-Query Attention is a memory efficient variant of Multi-Head Attention.\n",
    "Idea is to save memory by not creating a unique Key and Value for every single Query head. \n",
    "Instead, we just create a few K/V heads and have groups of Query heads share them. This is\n",
    "especially useful for longer sequences.\n",
    "\n",
    "The key difference from MHA is that the number of key/value heads is a hyperparameter, \n",
    "which must be a divisor of the number of query heads.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3da3ebde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'If': 0, 'attainable': 1, 'by': 2, 'humanly': 3, 'is': 4, 'it': 5, 'possible': 6, 'something': 7, 'too': 8, 'you': 9}\n",
      "Sentence IDs: tensor([0, 7, 4, 3, 6, 5, 4, 1, 2, 9, 8])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "sentence = 'If something is humanly possible, it is attainable by you too'\n",
    "\n",
    "dict = {s:i for i,s in enumerate(sorted(list(set(sentence.replace(',', '').split()))))}\n",
    "\n",
    "sentence_ids = torch.tensor([dict[s] for s in sentence.replace(',', '').split()])\n",
    "\n",
    "print(f\"Vocabulary: {dict}\")\n",
    "print(f\"Sentence IDs: {sentence_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84eb6b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 11, 32])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(dict)\n",
    "embed_dim = 32     \n",
    "num_q_heads = 8    \n",
    "num_kv_heads = 2  \n",
    "\n",
    "embedding_layer = nn.Embedding(vocab_size, embed_dim)\n",
    "embedded_input = embedding_layer(sentence_ids).unsqueeze(0) # Shape: [1, sequence length, embedding dim]\n",
    "\n",
    "print(f\"Input shape: {embedded_input.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b666cfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_q_heads, num_kv_heads):\n",
    "        super().__init__()\n",
    "        \n",
    "        # number of query heads should be a multiple of the KV heads\n",
    "        assert num_q_heads % num_kv_heads == 0, \"num_q_heads must be divisible by num_kv_heads\"\n",
    "        \n",
    "        self.num_q_heads = num_q_heads\n",
    "        self.num_kv_heads = num_kv_heads\n",
    "        self.head_dim = embed_dim // num_q_heads\n",
    "        self.num_groups = num_q_heads // num_kv_heads\n",
    "\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, self.num_kv_heads * self.head_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, self.num_kv_heads * self.head_dim)\n",
    "        \n",
    "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        \n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "        \n",
    "        # Reshape Q, K, V to separate heads\n",
    "        # Q,K,V shapes: [batch_size, num_q_heads, seq_len, head_dim]\n",
    "        q_heads = q.reshape(batch_size, seq_len, self.num_q_heads, self.head_dim).transpose(1, 2)\n",
    "        k_heads = k.reshape(batch_size, seq_len, self.num_kv_heads, self.head_dim).transpose(1, 2)\n",
    "        v_heads = v.reshape(batch_size, seq_len, self.num_kv_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Repeat K and V heads to match Q heads\n",
    "        k_heads = k_heads.repeat_interleave(self.num_groups, dim=1)\n",
    "        v_heads = v_heads.repeat_interleave(self.num_groups, dim=1)\n",
    "        \n",
    "        # Perform scaled dot-product attention\n",
    "        attn_scores = torch.matmul(q_heads, k_heads.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "        context = torch.matmul(attn_weights, v_heads)\n",
    "        \n",
    "        # Concatenate heads and apply final projection\n",
    "        # Reshape context to [batch_size, seq_len, d_model]\n",
    "        context = context.transpose(1, 2).reshape(batch_size, seq_len, d_model)\n",
    "        output = self.o_proj(context)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f088507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 11, 32])\n"
     ]
    }
   ],
   "source": [
    "gqa = GroupedQueryAttention(\n",
    "    embed_dim=embed_dim, \n",
    "    num_q_heads=num_q_heads, \n",
    "    num_kv_heads=num_kv_heads\n",
    ")\n",
    "\n",
    "output = gqa(embedded_input)\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
