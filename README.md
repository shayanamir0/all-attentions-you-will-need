# all attentions you will need
a collection of different attention mechanisms implemented from scratch

### checklist
- [x] Vanilla Self-Attention `self-attention.ipynb` [[paper]](https://arxiv.org/abs/1706.03762)
- [x] Grouped-Query Attention `gqa.ipynb` [[paper]](https://arxiv.org/abs/2305.13245)
